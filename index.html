<!DOCTYPE html>
<!-- saved from url=(0043)https://daooshee.github.io/BMVC2018website/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<meta name="viewport" content="width=960">
<title>BMVC2018 Deep Retinex Decomposition</title>
<link rel="stylesheet" type="text/css" href="./index_files/site.20180920212709.css">
<!--[if lte IE 7]>
<link rel="stylesheet" type="text/css" href="css/site.20180920212709-lteIE7.css">
<![endif]-->
</head>
<body id="body" data-new-gr-c-s-check-loaded="14.1106.0" data-gr-ext-installed="">
<div class="pos vis section">
<div class="vis-2 pos-2 size cont">
<p class="para"><span class="font">Multi-class Remote Sensing Image Semantic Segmentation with Graph-based Learning Metho</span></p>
</div>
<div class="vis-2 pos-3 size-2 cont-2">
<div class="vis-2 pos-4 size-3 cont-3">
<p class="para-2"><span class="font-2"><a href="https://github.com/raval137">Jaydev Raval *</a></span></p>
</div>
<div class="vis-2 pos-5 size-3 cont-4">
<p class="para-2"><span class="font-2"><a href="">Mohil Patel *</a></span></p>
</div>
<div class="vis-2 pos-6 size-3 cont-5">
<p class="para-2"><span class="font-2"><a href="">Nishit Gangani</a></span></p>
</div>
<div class="vis-2 pos-7 size-3 cont-6">
<p class="para-2"><span class="font-2"><a href="">Kushagra Mandaliya</a></span></p>
</div>
</div>
<div class="vis-2 pos-8 size-4 cont-7">
<p class="para-2"><span class="font-4"><a target="_blank" href="https://github.com/daooshee/BMVC2018website/blob/master/chen_bmvc18.pdf">Paper</a></span><span class="font-3"> | </span><span class="font-4"><a href="https://github.com/daooshee/BMVC2018website/blob/master/chen_bmvc18_sup.pdf">Report</a></span><span class="font-3"> | </span><span class="font-4"><a href="http://www.icst.pku.edu.cn/struct/Seminar/Talk_BMVC18_Chenwei/index.html">PPT</a></span></p>
<p class="para-3"><span class="font-5">* indicates equal contributions.</span></p>

</div>
<div class="vis-2 pos-9 size-5 cont-2">
<div class="vis-2 pos-4 size-5 colwrapper">
<div class="vis-2 pos-4 size-6 cont-8">
<picture class="img-2">
<img src="./index_files/Poster.png" alt="" width="400px" height="350px" style="vertical-align:middle;margin:0px 200px">
</picture>
</div>

<div style="float: right;">
<p class="para-4"><span class="font-3">Figure 1: Phases for the methodology of research; Phase 1: Data preprocessing invloves raw images converted into smaller patches; Phase 2: Clustering in which k is number of clusters and this clusters are used for feature extraction and graph generation; Phase 3: Model development, model trainingm, generate prediction and evalution</span></p>
</div>

</div>
</div>
<div class="vis-2 pos-11 size-8 cont">
<p class="para-5"><span class="font-6">Abstract</span></p>
<p class="para-4"><span class="font-3">Geo-spatial image segmentation is one of the most essential aspects of computer vision to analyze the objects in an image. It also applies to very high-resolution (VHR) imagery obtained from satellites to work on various research topics for many applications, including land-use classification, urban planning, and environmental monitoring. This research introduces the fundamentals of remote sensing images or satellite images and image segmentation with them. Followed by, understanding the requirement of deep learning (DL) for this application and reviewing existing literature which demonstrates work done in the field using graph neural networks (GNN) and other advanced architectures like the graph attention network (GAT), the self-constructing graph neural network (SCG-NN), and the graph u-net. This paper proposes a graph-based deep learning-based approach for geo-spatial image segmentation using GAT with multiple heads and channel-based feature extraction. A state-of-the-art DL architecture for graph-based image segmentation incorporates an attention layer to enhance the model's ability to focus on essential features and regions in the image. The analysis of results reveals that the proposed model achieved an F1 score of 80.80 on the Potsdam dataset, whereas the state-of-the-art model achieved an F1 score of 92.7 on same dataset. It's important to note, though, that there's still room for improvement in terms of results.</span></p>
<p class="para-4"><span class="font-3">&nbsp;</span></p>
</div>
<div>
  <p class="para-5"><span class="font-6">Phase 1</span></p>
  <p class="para-4"><span class="font-3">Potsdam satellite images or tiles are 6000 x 6000 in dimension, and a total of 38 images are utilized. From these large dimension images, multiple patches of 512 x 512 dimension are created. These resulting images are then split into training, testing, and validation sets, with 19 images used for training, 4 images used for validation, and 15 images used for testing. Annotations of the label are provided which are also changed into patches similar to the input image size.</span></p>
  <p class="para-4"><span class="font-3">&nbsp;</span></p>
</div>

<div>
  <p class="para-5"><span class="font-6">Phase 2</span></p>
  <p class="para-4"><span class="font-3">SLIC is a clustering algorithm commonly used for this type of research problem. The basic idea of SLIC is to group up pixels in an image based on their spatial proximity and color similarity. This is achieved by defining a set of "superpixels", which are compact, roughly equally sized regions of the image that contain similar colors. Each pixel in the image is then assigned to its nearest superpixel based on both its color and spatial proximity to the superpixel.</span></p>
  <p class="para-4"><span class="font-3">After clustering, necessary features need to be retrieved from the cluster, for that, histogram method is used. From the RGB image, different number of quantile bins are defined for the histogram, which determines the granularity of the color information that will be captured. A larger number of bins will capture more detailed color information. For each cluster, count the number of pixels that belong to each color bin. This can be done by looping over all the pixels in the cluster and incrementing the corresponding bin in the histogram for each pixel. To account for the fact that different clusters may have different sizes, the histogram should be normalized so that it represents a probability distribution. This can be done by dividing each bin count by the total number of pixels in the cluster.
    From the resulting color histogram, some of the quantiles are selected as the features extracted for that cluster. For our experiment, 5th, 25th, 50th, 75th and 95th quantiles are used. Using these, graphs have been generated and further will be used for model development.</span></p>
  <p class="para-4"><span class="font-3">The cluster formed in the feature extraction are represented as nodes, and each can be labeled according to the cluster ID, The edges between nodes represent the similarity between the corresponding color histograms. One common approach is to use a distance metric, to measure the distance between the nodes. We are considering 5 neighboring nodes to form a cluster. The edges can then be weighted, for our case, we are considering same weight for each edge. Each node is represented as N which consists of a set of id, features, label, where id is the unique identifier for each node, features are extracted using a quantile-based approach as mentioned earlier, and label refers to the class the node belongs to.

    Once the nodes N and edges E have been defined, they can be used to construct the graph. This can be done using a deep graph library and it is represented as G(N, E) which consists of information of nodes N, edges E and metadata type of graph (direction or undirection) etc.</span></p>
  <p class="para-4"><span class="font-3">&nbsp;</span></p>
</div>

<div>
  <p class="para-5"><span class="font-6">Phase 3</span></p>
  <p class="para-4"><span class="font-3">The main idea behind GAT is to compute a node embedding by aggregating the embeddings of its neighboring nodes. The aggregation process is performed using an attention mechanism, which assigns weights to each neighboring node based on its relevance to the current node. The score is then normalized using the softmax function, and the normalized scores are used as weights for the aggregation operation.</span></p>
  <p class="para-4"><span class="font-3">From the graph generation, we have specified quantiles of 5%, 25%, 50%, 75%, 95% and input image with 3 color channels, we have 15 input features per node, as we have specified 2000 cluster to be formed using SLIC and compactness set as 0.1, the learning rate is initially set to  0.0001, weight decay is 0.0001 and learning rate decay is 0.98.
    
    The number of layers used is 10 with neurons in layers are [512, 512, 256, 256, 128, 128, 64, 64, 32, 32] and along with each layer having 6 attention layers that are concatenated with graph convolutional layers, each layer is connected with the exponential linear unit. To train the model, each layer is connected with elu activation function and the last layer has softmax which provides the final prediction output.
    
    This model is set for 200 epochs, but to avoid overfitting, early stopping is implemented on validation loss with the patience parameter set at 10, hence, model exit training at 35-40 epochs. The model takes approximately 14-15 hours to completely train and gives a prediction on the testing set with a total trainable parameter of 19040274. All the experiments were conducted on narval cluster of Compute Canada</span></p>
  <p class="para-4"><span class="font-3">&nbsp;</span></p>
</div>

<div>
  <p class="para-5"><span class="font-6">Results</span></p>
</div>

<div>
<picture class="img-6">
<img src="./index_files/ConferenceQuantative.png" width="800px" height="350px" style="vertical-align:middle;margin:0px 100px">
</picture>
<p class="para-4"><span class="font-3">The research results were produced by reproducing the distribution of the dataset for training, validation, and testing, out of 38 tiles, 19 tiles were used for training, 4 tiles were used for testing and 15 tiles were used for validation, the ratio of splitting tiles was followed with reference to \cite{Diao2022SuperpixelBasedAG}, while other parameters were determined by experimental results and different model configuration and comparing among them.

  Here, the results are compared between the state-of-the-art models and the current research models. \cite{He2021RSINetTD} achieved an average F1 score of 91.49\% while using a fusion of GCN and CNN for feature extraction at different encoder and decoder levels, with different results based on different parameters. \cite{Diao2022SuperpixelBasedAG} has a mean F1 score of 92.01\% on the Potsdam dataset, where CNN and GCN are used in a linear architecture unlike the previous model as it also improves results. Using the SCG-NN with attention head to obtain an F1 Score of 92.7\%, this approach for the existing model is different than what is proposed in this paper \cite{Zi2021SGANetSG}.
  
  In this research, we have used a new mechanism with the use of a multi-attention head, which is not been used by any other existing models on the Potsdam benchmark dataset. While using 2 different models, one with a U architecture of connection between pooling and unpooling layer with a depth network. Another is with a traditional GCN, without and CNN prior connection to the model as compared to existing work, with multi-attention heads with different series of layers to get the optimal solution.</span></p>

<picture class="img-6">
  <img src="./index_files/resultGraphv2.png" width="450px" height="350px" style="vertical-align:middle;margin:0px 200px">
  </picture>
  <p class="para-4"><span class="font-3">This analysis consists of a visual study of test results from the trained model. From the partition of the dataset for the testing model, 24 patches are selected, and for each patch, the actual RGB image, ground truth, and prediction are shown in Fig. \ref{fig:quantitative}. Upon close inspection of the images, prediction images have issues with the distortion of the labels observed around the edges of the label. While imaging 2\_10\_RGB\_93 overlap of the background and tree is predicted where as per ground truth, also in the predicted image there is a fine classification for low vegetation, whereas the growth truth has labeled the whole region as low vegetation. Upon further inspection, the majority of mislabeling occurred between the low vegetation and trees, trees, and background./span></p>
</div>
</div>

<div class="vis-2 pos-14 size-16 cont-13">
<p class="para-5"><span class="font-6">Download Links</span><span class="font-3">&nbsp;</span></p>
<ul class="pos-21">
<li class="para-6"><span class="font-7">• </span><span class="font-7">Datasets</span></li>
</ul>
<p class="para-4"><span class="font-3">&nbsp;&nbsp;&nbsp;&nbsp;LOw Light paired dataset (LOL): </span><span class="font-4"><a href="https://drive.google.com/open?id=157bjO1_cFuSd0HWDUuAmcHRJDVyWpOxB">Google Drive</a></span><span class="font-3">, </span><span class="font-4"><a href="https://pan.baidu.com/s/1ABMrDjBTeHIJGlOFIeP1IQ">Baidu Pan (Code:acp3)</a></span></p>
<p class="para-4"><span class="font-3">&nbsp;&nbsp;&nbsp;&nbsp;Synthetic Image Pairs from Raw Images: </span><span class="font-4"><a href="https://drive.google.com/open?id=1G6fi9Kiu7CDnW2Sh7UQ5ikvScRv8Q14F">Google Drive</a></span><span class="font-3">, </span><span class="font-4"><a href="https://pan.baidu.com/s/1drsMAkRMlwd9vObAM_9Iog">Baidu Pan</a></span></p>
<p class="para-4"><span class="font-3">&nbsp;&nbsp;&nbsp;&nbsp;Testing Images: </span><span class="font-4"><a href="https://drive.google.com/open?id=1OvHuzPBZRBMDWV5AKI-TtIxPCYY8EW70">Google Drive</a></span><span class="font-3">, </span><span class="font-4"><a href="https://pan.baidu.com/s/1G2qg3oS12MmP8_dFlVRRug">Baidu Pan</a></span></p>
<ul class="pos-21">
<li class="para-6"><span class="font-7">• </span><span class="font-7">Codes</span></li>
</ul>
</div>
</div>
<script type="text/javascript" src="./index_files/jquery.js.download"></script>
<script type="text/javascript" src="./index_files/index.20180920212709.js.download"></script>
<script type="text/javascript">
var ver=RegExp(/Mozilla\/5\.0 \(Linux; .; Android ([\d.]+)/).exec(navigator.userAgent);if(ver&&parseFloat(ver[1])<5){document.getElementsByTagName('body')[0].className+=' whitespacefix';}
</script>


<div id="torrent-scanner-popup" style="display: none;"><template shadowrootmode="open"><link rel="stylesheet" href="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/css/custom.css"><div id="yf-bt-wrapper" class="free"><div class="header"><img class="sts-logo" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/ts-free-logo.png"><div class="search-content"><input id="search-input" class="search-input" type="search" placeholder="Start your search here..."><span id="search-btn" class="search-btn"></span></div></div><div class="container"><div class="main-container"><div id="torrent-data" class="torrent-content"><div class="t-table"><div class="t-header"><div class="t-name">Torrent search results</div></div><div id="checked-sites" class="checked-sites-section"><div class="left">Checked Sites</div><div class="right"><span id="sites-count" class="sites-count">0</span></div></div><div id="table-body" class="t-body"><div id="loading" class="spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><div class="table-message-container" id="table-message"><p>No items to list <br> Use the search bar above for instant results</p></div></div></div></div><div class="tooltip"><p class="tooltip-text">To see search results, type here and hit `Enter`</p></div><div class="footer"><span><span id="numberScanned" class="numberScanned">No results</span></span></div><div class="upgradeProPanel"><div class="upgradeProPanelTitle">Try our Pro Versions to unlock:</div><div class="upgradeProPanelList"><div><p>Faster Results</p></div><div><p>Secure Torrenting</p></div><div><p>Unlimited Search Results with detailed torrent info</p></div><div><p>1-YR Subscription to CyberGhost VPN <span style="font-size:8px;">(PRO+VPN only)</span></p></div></div><a class="upgrade-to-pro-button-2" id="buy-pro-vpn" href="https://www.utorrent.com/webpro-offer/?utm_source=Lavasoft&amp;utm_medium=version_1.0&amp;utm_campaign=Scanner" target="_blank">BUY PRO + VPN</a><a class="upgrade-to-pro-button-2" id="buy-pro" href="https://www.utorrent.com/webpro-offer/?utm_source=Lavasoft&amp;utm_medium=version_1.0&amp;utm_campaign=Scanner" target="_blank">BUY PRO</a></div></div><div class="sync-container nav-se-container"><div class="nav-se-content"><img class="sync-icon nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-sync.svg"><div class="nav-se-title">One more step to go before you start torrenting!</div><p class="nav-se-text">This extension can sync results with BitTorrent and/or uTorrent for instant downloading.</p><p class="nav-se-text">To activate this feature, please click on the button below, and then on the Chrome message to activate the 'Messaging Permission'.</p><button class="sync-permission-btn nav-se-btn">Activate Messaging Permission</button></div><div class="nav-se-content display-none"><img class="sync-icon nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-sync.svg"><div class="nav-se-title">Syncing...</div><p class="nav-se-text">Please allow Messaging Permissions in the proceeding Chrome message.</p></div><div class="nav-se-content display-none"><img class="sync-icon nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-success.svg"><div class="nav-se-title">Sync Complete</div><p class="nav-se-text">You have successfully activated the “Messaging Permission” feature. All your search results will sync with BitTorrent and/or uTorrent.</p></div></div><div class="license-container nav-se-container"><div class="nav-se-content"><img class="nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-key2.svg"><div class="nav-se-title">Enter License Key</div><p class="nav-se-text">Enter your license key and click on the activate button to start using <span>µTorrent Web Pro.</span></p><input type="text" id="license-input-key" class="license-input-key" placeholder="Enter Key"><div class="license-spinner"><div class="bounce1"></div><div class="bounce2"></div><div class="bounce3"></div></div><button id="license-activate-button" class="license-activate-button nav-se-btn">Activate</button><p>Don't have a license key? <a class="license-buy-link" target="_blank">Click here</a></p></div><div class="nav-se-content display-none"><img class="nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-success.svg"><div class="nav-se-title">Happy Torrenting!</div><p class="nav-se-text">You are now an active PRO user</p><p class="nav-se-text">Your key is valid until <span class="expiry-date"></span></p></div><div class="nav-se-content display-none"><img class="nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-alert.svg"><div class="nav-se-title">Your license key has expired</div><p class="nav-se-text">Looks like your license key has expired, to renew your PRO license key, please select a license type:</p><a class="upgrade-to-pro-button-2 buy-license-expiry-button" href="https://www.utorrent.com/webpro-offer/?utm_source=Lavasoft&amp;utm_medium=version_1.0&amp;utm_campaign=Scanner" target="_blank">Buy Pro + VPN</a><a class="upgrade-to-pro-button-2 buy-license-expiry-button" href="https://www.utorrent.com/webpro-offer/?utm_source=Lavasoft&amp;utm_medium=version_1.0&amp;utm_campaign=Scanner" target="_blank">Buy Pro</a><p class="nav-se-text">Already have a license key? <a class="link" id="show-license-panel">Click here</a></p></div><div class="nav-se-content display-none"><img class="nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-success.svg"><div class="nav-se-title">Pro User</div><p class="nav-se-text">Your key is valid until <span class="expiry-date"></span></p><p class="nav-se-text">Your License Key:</p><p class="nav-se-text"></p><p class="nav-se-text margin-top-50">Switch back to Torrent Scanner Free?</p><button class="activate-free-btn nav-se-btn">Revert to Free Version</button></div></div><div class="feedback-container nav-se-container"><div class="nav-se-content"><img class="nav-se-icon" src="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/img/assets/icon-feedback.svg"><div class="nav-se-title">Feedback</div><p class="nav-se-text">Help us improve Torrent Scanner, send us comments, bugs, feedback, and suggestions.</p><button id="feedback-button" class="feedback-button nav-se-btn">Send Feedback</button></div></div><div class="settings-container nav-se-container"><div class="nav-se-content"><div class="settings-title">Settings</div></div><div class="s-table"><div class="s-row"><div class="s-title">FAQ<div class="s-arrow"><span class="arrow-down"></span></div></div><div class="s-content faq-content"><p class="faq-text">FAQ: <a href="chrome-extension://aegnopegbbhjeeiganiajffnalhlkkjb/faq.html" target="_blank">Click here</a></p></div></div><div class="s-row"><div class="s-title">Rate the extension<div class="s-arrow"><span class="arrow-down"></span></div></div><div class="s-content"><p class="rate-text">How did you like the extension experience?</p><div class="rating"><span class="rating-star">★</span><span class="rating-star">★</span><span class="rating-star">★</span><span class="rating-star">★</span><span class="rating-star">★</span></div><button class="rating-btn nav-se-btn" disabled="">Submit</button></div></div><div class="s-row"><div class="s-title">About<div class="s-arrow"><span class="arrow-down"></span></div></div><div class="s-content about-content"><div class="about-version">Version 1.3.0 <br><br> What's New</div><div class="about-new"><p></p><ul><li>Experience a complete new User Interface of the extension. It is enhanced and user friendly now.</li><li>Squashed some bugs.</li></ul><p></p></div></div></div><div class="s-row"><div class="s-title">Privacy Policy<div class="s-arrow"><span class="arrow-down"></span></div></div><div class="s-content"><div class="policy-text">Adaware Software (7270356 Canada Inc.) is the operator of the Adaware products suites and related services (the “<b>Company</b>”, ”<b>we</b>” or “<b>us</b>”). We respect your privacy rights and we are committed to protecting them. This privacy policy (“<b>Privacy Policy</b>” or simply “<b>policy</b>”) governs our products, services and websites that link to this Privacy Policy, and describes our practices of processing data from you. By “<b>you</b>”, we refer to either or all of the following: (i) visitors to our websites that links to this Privacy Policy (“<b>Visitor</b>” and “<b>Website</b>”, respectively); (ii) our customers using our software products and Services (“<b>User</b>”); and (c) a business customer, a business partner that has a contractual relationship with us or a prospective customer that is yet to be engaged in a contract with us (“Business Customer”). Unless explicitly mentioned otherwise, the information in this Privacy Policy refers to any and all data subject types (“you” or “your’). <br><br> For the purpose of this policy, the “<b>Service(s)</b>” shall include any software licensed by the Company, including features offered by or within the installed software or additional software scripts available therein (either downloaded from one of our websites, pre-installed on your device, downloaded through a third party website, obtained on a physical medium, or otherwise), or services provided through and/or on top such software, services offered on our websites, communication forums, support services, account operation, updates, enhancements, new features, premium support, extended guarantees, online version and free versions of a software or additional services or features as we ay make available from time to time. <br><br> If you are a California resident, please also see our <a href="https://www.adaware.com/CCPA/" target="_blank">CCPA Notice</a>. <br><br> <a href="https://www.adaware.com/privacy-policy/" target="_blank">Read more</a></div></div></div><div class="s-row" style="display: none;"><div class="s-title">Contact Us<div class="s-arrow"><span class="arrow-down"></span></div></div><div class="s-content"><div class="contact-text">For any payment and order-related support, please contact us at Email: <a href="mailto:support@torrentscanner.zendesk.com">support@torrentscanner.zendesk.com</a> or <a href="mailto:pcsoftwareinfo.com">pcsoftwareinfo.com</a><br><br>Phone: <a href="https://pcsoftwareinfo.com/contact.aspx" target="_blank">Click here</a></div></div></div></div></div></div><div class="nav"><button id="btnSync" class="nav-btn">Sync</button><button id="btnLicense" class="nav-btn">License</button><button id="btnHome" class="nav-btn">Home</button><button id="btnFeedback" class="nav-btn">Feedback</button><button id="btnSettings" class="nav-btn">Settings</button></div></div></template></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
  div.grammarly-desktop-integration {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
    -moz-user-select: none;
    -webkit-user-select: none;
    -ms-user-select:none;
    user-select:none;
  }

  div.grammarly-desktop-integration:before {
    content: attr(data-content);
  }
</style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>
